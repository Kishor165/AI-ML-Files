{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36c5ff4e-cdc7-44d3-84a8-fb76ff57e038",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+\n|   Name|Age|\n+-------+---+\n|  Alice| 25|\n|    Bob| 30|\n|Charlie| 35|\n+-------+---+\n\n"
     ]
    }
   ],
   "source": [
    "# Import PySpark and initialize Spark session \n",
    "import pyspark \n",
    "from pyspark.sql import SparkSession \n",
    "# Create a Spark session \n",
    "spark = SparkSession.builder.appName(\"PySparkExample\").getOrCreate() \n",
    "# Create a DataFrame with sample data \n",
    "data = [(\"Alice\", 25), (\"Bob\", 30), (\"Charlie\", 35)] \n",
    "df = spark.createDataFrame(data, [\"Name\", \"Age\"]) \n",
    "# Show the DataFrame \n",
    "df.show() \n",
    "# Stop the Spark session \n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2fcd798-ff23-4603-acb6-259e8a7d8c40",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mPySparkRuntimeError\u001B[0m                       Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-5036194371013616>, line 4\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msql\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SparkSession\n",
       "\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m# Create a Spark session\u001B[39;00m\n",
       "\u001B[0;32m----> 4\u001B[0m spark \u001B[38;5;241m=\u001B[39m SparkSession\u001B[38;5;241m.\u001B[39mbuilder\u001B[38;5;241m.\u001B[39mappName(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMyApp\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mgetOrCreate()\n",
       "\u001B[1;32m      6\u001B[0m \u001B[38;5;66;03m# Read the CSV file\u001B[39;00m\n",
       "\u001B[1;32m      7\u001B[0m df \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mread\u001B[38;5;241m.\u001B[39mcsv(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/Volumes/kishor1/default/spark/employees1(2).csv\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/session.py:547\u001B[0m, in \u001B[0;36mSparkSession.Builder.getOrCreate\u001B[0;34m(self)\u001B[0m\n",
       "\u001B[1;32m    542\u001B[0m             \u001B[38;5;28;01mreturn\u001B[39;00m cast(\n",
       "\u001B[1;32m    543\u001B[0m                 SparkSession,\n",
       "\u001B[1;32m    544\u001B[0m                 RemoteSparkSession\u001B[38;5;241m.\u001B[39mbuilder\u001B[38;5;241m.\u001B[39mconfig(\u001B[38;5;28mmap\u001B[39m\u001B[38;5;241m=\u001B[39mopts)\u001B[38;5;241m.\u001B[39mgetOrCreate(),\n",
       "\u001B[1;32m    545\u001B[0m             )\n",
       "\u001B[1;32m    546\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[0;32m--> 547\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m PySparkRuntimeError(\n",
       "\u001B[1;32m    548\u001B[0m                 error_class\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSESSION_ALREADY_EXIST\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m    549\u001B[0m                 message_parameters\u001B[38;5;241m=\u001B[39m{},\n",
       "\u001B[1;32m    550\u001B[0m             )\n",
       "\u001B[1;32m    552\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n",
       "\u001B[1;32m    553\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mOnly remote Spark sessions using Databricks Connect are supported. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m    554\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUse DatabricksSession.builder to create a remote Spark session instead.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m    555\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRefer to https://docs.databricks.com/dev-tools/databricks-connect.html \u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m    556\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mon how to configure Databricks Connect.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m    557\u001B[0m )\n",
       "\u001B[1;32m    559\u001B[0m session \u001B[38;5;241m=\u001B[39m SparkSession\u001B[38;5;241m.\u001B[39m_instantiatedSession\n",
       "\n",
       "\u001B[0;31mPySparkRuntimeError\u001B[0m: [SESSION_ALREADY_EXIST] Cannot start a remote Spark session because there is a regular Spark session already running."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "PySparkRuntimeError",
        "evalue": "[SESSION_ALREADY_EXIST] Cannot start a remote Spark session because there is a regular Spark session already running."
       },
       "metadata": {
        "errorSummary": "[SESSION_ALREADY_EXIST] Cannot start a remote Spark session because there is a regular Spark session already running."
       },
       "removedWidgets": [],
       "sqlProps": {
        "errorClass": "SESSION_ALREADY_EXIST",
        "pysparkCallSite": "",
        "pysparkFragment": "",
        "pysparkSummary": null,
        "sqlState": null,
        "stackTrace": null,
        "startIndex": null,
        "stopIndex": null
       },
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mPySparkRuntimeError\u001B[0m                       Traceback (most recent call last)",
        "File \u001B[0;32m<command-5036194371013616>, line 4\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msql\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SparkSession\n\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m# Create a Spark session\u001B[39;00m\n\u001B[0;32m----> 4\u001B[0m spark \u001B[38;5;241m=\u001B[39m SparkSession\u001B[38;5;241m.\u001B[39mbuilder\u001B[38;5;241m.\u001B[39mappName(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMyApp\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mgetOrCreate()\n\u001B[1;32m      6\u001B[0m \u001B[38;5;66;03m# Read the CSV file\u001B[39;00m\n\u001B[1;32m      7\u001B[0m df \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mread\u001B[38;5;241m.\u001B[39mcsv(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/Volumes/kishor1/default/spark/employees1(2).csv\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/session.py:547\u001B[0m, in \u001B[0;36mSparkSession.Builder.getOrCreate\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    542\u001B[0m             \u001B[38;5;28;01mreturn\u001B[39;00m cast(\n\u001B[1;32m    543\u001B[0m                 SparkSession,\n\u001B[1;32m    544\u001B[0m                 RemoteSparkSession\u001B[38;5;241m.\u001B[39mbuilder\u001B[38;5;241m.\u001B[39mconfig(\u001B[38;5;28mmap\u001B[39m\u001B[38;5;241m=\u001B[39mopts)\u001B[38;5;241m.\u001B[39mgetOrCreate(),\n\u001B[1;32m    545\u001B[0m             )\n\u001B[1;32m    546\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 547\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m PySparkRuntimeError(\n\u001B[1;32m    548\u001B[0m                 error_class\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSESSION_ALREADY_EXIST\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    549\u001B[0m                 message_parameters\u001B[38;5;241m=\u001B[39m{},\n\u001B[1;32m    550\u001B[0m             )\n\u001B[1;32m    552\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[1;32m    553\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mOnly remote Spark sessions using Databricks Connect are supported. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    554\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUse DatabricksSession.builder to create a remote Spark session instead.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    555\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRefer to https://docs.databricks.com/dev-tools/databricks-connect.html \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    556\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mon how to configure Databricks Connect.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    557\u001B[0m )\n\u001B[1;32m    559\u001B[0m session \u001B[38;5;241m=\u001B[39m SparkSession\u001B[38;5;241m.\u001B[39m_instantiatedSession\n",
        "\u001B[0;31mPySparkRuntimeError\u001B[0m: [SESSION_ALREADY_EXIST] Cannot start a remote Spark session because there is a regular Spark session already running."
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"MyApp\").getOrCreate()\n",
    "\n",
    "# Read the CSV file\n",
    "df = spark.read.csv(\"/Volumes/kishor1/default/spark/employees1(2).csv\")\n",
    "\n",
    "# Display the DataFrame\n",
    "display(df)\n",
    "\n",
    "# Print the schema of the DataFrame\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a9123240-698d-4e78-8d39-e434a960ffc2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "pyspark1",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}